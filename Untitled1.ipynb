{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dbb764c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:811: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:811: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'detectron2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19560\\588028065.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDatasetFromList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstructures\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBoxMode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBoxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'detectron2'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import argparse\n",
    "import base64\n",
    "from detectron2.data import DatasetFromList, MapDataset\n",
    "from detectron2.structures import BoxMode,Boxes\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data.build import DatasetMapper, trivial_batch_collator\n",
    "from OCR import OCR\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from fuzzywuzzy.process import default_processor\n",
    "import cfg\n",
    "import cfg_head\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from plain_train_net import ValidationSampler\n",
    "from train_net import RegularTrainer,Trainer\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "from GCV import gcv_ocr\n",
    "from relation_data_tool import PathwayDatasetMapper\n",
    "from shape_tool import relation_covers_this_element\n",
    "from formulate_relation import get_subimg, translation_transform_on_element_bbox, perspective_transform_on_element_bbox\\\n",
    "    ,find_largest_area_symbols,find_vertex_for_detected_relation_symbol_by_distance,dist_center,find_best_text,\\\n",
    "    center_point_in_box,calculate_distance_between_two_boxes\n",
    "from swaps import swaps\n",
    "from nfkc import nfkc\n",
    "from deburr import deburr\n",
    "from upper import upper\n",
    "from expand import expand\n",
    "# write a function that loads the dataset into detectron2's standard format\n",
    "def get_data_dicts(img_path):\n",
    "    # go through all label files\n",
    "    dataset_dicts = []\n",
    "\n",
    "    for idx, img_file in enumerate(os.listdir(img_path)):\n",
    "        try:\n",
    "\n",
    "            # read key and value from current json file\n",
    "            filename = os.path.join(img_path, img_file)\n",
    "            img = cv2.imread(filename)\n",
    "            height, width = img.shape[:2]\n",
    "            del img\n",
    "        except Exception as e:\n",
    "            # print(str(e))\n",
    "            continue\n",
    "\n",
    "        # declare a dict variant to save the content\n",
    "        record = {}\n",
    "\n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        record[\"annotations\"] = None\n",
    "        dataset_dicts.append(record)\n",
    "\n",
    "    return dataset_dicts\n",
    "\n",
    "\n",
    "def build_data_fold_loader(cfg, data_folder, mapper=None):\n",
    "    \"\"\"\n",
    "    Similar to `build_detection_test_loader`.\n",
    "    But this function uses the given `dataset_name` argument (instead of the names in cfg),\n",
    "\n",
    "    Args:\n",
    "        cfg: a detectron2 CfgNode\n",
    "        data_folder (str): folder includes data\n",
    "        mapper (callable): a callable which takes a sample (dict) from dataset\n",
    "           and returns the format to be consumed by the model.\n",
    "           By default it will be `DatasetMapper(cfg, False)`.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: a torch DataLoader, that loads the given detection\n",
    "        dataset, with test-time transformation and batching.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_dicts = list(itertools.chain.from_iterable([get_data_dicts(data_folder)]))\n",
    "    # dataset_dicts = list(itertools.chain.from_iterable(get_data_dicts(data_folder)))\n",
    "    dataset = DatasetFromList(dataset_dicts)\n",
    "    if mapper is None:\n",
    "        mapper = DatasetMapper(cfg, False)\n",
    "    dataset = MapDataset(dataset, mapper)\n",
    "\n",
    "    sampler = ValidationSampler(len(dataset))\n",
    "\n",
    "    batch_sampler = torch.utils.data.sampler.BatchSampler(sampler, cfg.SOLVER.IMS_PER_BATCH, drop_last=False)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        num_workers=cfg.DATALOADER.NUM_WORKERS,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=trivial_batch_collator,\n",
    "    )\n",
    "    # del dataset_dicts, dataset\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def instances_to_coco_json(instances, img_id, file_name):\n",
    "    \"\"\"\n",
    "    Dump an \"Instances\" object to a COCO-format json that's used for evaluation.\n",
    "\n",
    "    Args:\n",
    "        instances (Instances):\n",
    "        img_id (int): the image id\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: list of json annotations in COCO format.\n",
    "    \"\"\"\n",
    "    num_instance = len(instances)\n",
    "    if num_instance == 0:\n",
    "        return []\n",
    "\n",
    "    boxes = instances.pred_boxes.tensor.numpy()\n",
    "    boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)\n",
    "    boxes = boxes.tolist()\n",
    "    scores = instances.scores.tolist()\n",
    "    classes = instances.pred_classes.tolist()\n",
    "    results = []\n",
    "    for k in range(num_instance):\n",
    "        result = {\n",
    "            \"image_id\": img_id,\n",
    "            \"file_name\": file_name,\n",
    "            \"category_id\": classes[k],\n",
    "            \"bbox\": boxes[k],\n",
    "            \"score\": scores[k],\n",
    "        }\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def inference_on_dataset(model, data_loader):\n",
    "    \"\"\"\n",
    "    Run model on the data_loader and evaluate the metrics with evaluator.\n",
    "    The model will be used in eval mode.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): a module which accepts an object from\n",
    "            `data_loader` and returns some outputs. It will be temporarily set to `eval` mode.\n",
    "\n",
    "            If you wish to evaluate a model in `training` mode instead, you can\n",
    "            wrap the given model and override its behavior of `.eval()` and `.train()`.\n",
    "        data_loader: an iterable object with a length.\n",
    "            The elements it generates will be the inputs to the model.\n",
    "        evaluator (DatasetEvaluator): the evaluator to run. Use\n",
    "            :class:`DatasetEvaluators([])` if you only want to benchmark, but\n",
    "            don't want to do any evaluation.\n",
    "\n",
    "    Returns:\n",
    "        The return value of `evaluator.evaluate()`\n",
    "    \"\"\"\n",
    "    # num_devices = torch.distributed.get_world_size() if torch.distributed.is_initialized() else 1\n",
    "    # logger = logging.getLogger(__name__)\n",
    "    # logger.info(\"Start inference on {} images\".format(len(data_loader)))\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    predictions = []\n",
    "    with inference_context(model.to(device)), torch.no_grad():\n",
    "        for idx, inputs in enumerate(data_loader):\n",
    "            # print('&&&&&',idx,inputs)\n",
    "            output = model.to(device)(inputs)\n",
    "            instances = output[0][\"instances\"].to(cpu_device)\n",
    "            prediction = instances_to_coco_json(instances, inputs[0][\"image_id\"], inputs[0]['file_name'])\n",
    "            predictions.extend(prediction)\n",
    "            del prediction\n",
    "            # print(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def inference_context(model):\n",
    "    \"\"\"\n",
    "    A context where the model is temporarily changed to eval mode,\n",
    "    and restored to previous mode afterwards.\n",
    "\n",
    "    Args:\n",
    "        model: a torch Module\n",
    "    \"\"\"\n",
    "    training_mode = model.training\n",
    "    model.eval()\n",
    "    yield\n",
    "    model.train(training_mode)\n",
    "\n",
    "\n",
    "# def setup(cfg_file_path):\n",
    "#     \"\"\"\n",
    "#     Create configs and perform basic setups.\n",
    "#     \"\"\"\n",
    "#     cfg = get_cfg()\n",
    "#     cfg.merge_from_file(cfg_file_path)\n",
    "#     cfg.OUTPUT_DIR = os.path.join(r'./output/interface/')\n",
    "#     # cfg.test_home_folder =kwargs['dataset']\n",
    "#     cfg.freeze()\n",
    "#\n",
    "#     return cfg\n",
    "\n",
    "def setup(cfg, kwargs):\n",
    "    \"\"\"\n",
    "    Create configs and perform basic setups.\n",
    "    \"\"\"\n",
    "    configuration = get_cfg()\n",
    "    configuration.merge_from_file(cfg.element_config_file)\n",
    "    # configuration.OUTPUT_DIR = os.path.join(r'./output/interface/')\n",
    "    # configuration.test_home_folder = kwargs['dataset']\n",
    "    # print('cfg.test_home_folder :',configuration.test_home_folder)\n",
    "    configuration.freeze()\n",
    "\n",
    "    return configuration\n",
    "\n",
    "\n",
    "def predict(cfg_file_path,entity_type,data_folder):\n",
    "    config =get_cfg()\n",
    "    config.merge_from_file(cfg_file_path)\n",
    "    config.freeze()\n",
    "\n",
    "    if entity_type!= 'rotated_relation':\n",
    "        model = RegularTrainer.build_model(config)\n",
    "        DetectionCheckpointer(model=model,\n",
    "                              save_dir=config.OUTPUT_DIR).resume_or_load(cfg.relation_model, resume=False)\n",
    "\n",
    "        data_loader = build_data_fold_loader(config, data_folder, mapper=DatasetMapper(config, False))\n",
    "\n",
    "        #img_size = (data_loader.dataset[0]['height'], data_loader.dataset[0]['width'])\n",
    "        predictions = inference_on_dataset(model, data_loader)\n",
    "        # evaluation_res = RegularTrainer.test(config, model,\n",
    "        #              RegularEvaluator(config.DATASETS.TEST[0], config,\n",
    "        #                  True, False, config.OUTPUT_DIR))\n",
    "        #pass\n",
    "\n",
    "    else:\n",
    "        model = Trainer.build_model(config)\n",
    "        DetectionCheckpointer(model=model,\n",
    "                              save_dir=config.OUTPUT_DIR).resume_or_load(\n",
    "            os.path.join(config.OUTPUT_DIR, cfg.rotated_relation_model), resume=False)\n",
    "\n",
    "        data_loader = build_data_fold_loader(config, data_folder, mapper=PathwayDatasetMapper(config, False))\n",
    "        #img_size = (data_loader.dataset[0]['height'], data_loader.dataset[0]['width'])\n",
    "        predictions = inference_on_dataset(model, data_loader)\n",
    "\n",
    "    del model,data_loader\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def normalize_rect_vertex(points, image_size):\n",
    "    if len(points) == 4:\n",
    "        boxes = np.array(points, np.float).reshape((-1, 4))\n",
    "        boxes = BoxMode.convert(boxes, BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n",
    "\n",
    "        boxes = Boxes(boxes)\n",
    "        boxes.clip(image_size)\n",
    "\n",
    "        points = np.array(boxes.tensor).reshape((2, 2))\n",
    "        # print('point',points)\n",
    "        pt0 = np.min(points[:, 0])\n",
    "        pt1 = np.min(points[:, 1])\n",
    "        pt4 = np.max(points[:, 0])\n",
    "        pt5 = np.max(points[:, 1])\n",
    "        pt2 = pt4\n",
    "        pt3 = pt1\n",
    "        pt6 = pt0\n",
    "        pt7 = pt5\n",
    "        del points, boxes\n",
    "        return np.array([[pt0, pt1], [pt2, pt3], [pt4, pt5], [pt6, pt7]], np.int32).reshape((4, 2))\n",
    "    if len(points) == 5:\n",
    "        cnt_x, cnt_y, w, h, angle = points\n",
    "        return np.array(cv2.boxPoints(((cnt_x, cnt_y),(w, h), angle)), np.int32).reshape((4, 2))\n",
    "\n",
    "\n",
    "\n",
    "def normalize_all_boxes(prediction_instances, image_size):\n",
    "    assert 'normalized_bbox' in  prediction_instances.columns.values\n",
    "    for row_idx in range(0, len(prediction_instances)):\n",
    "        prediction_instances._set_value(prediction_instances.index[row_idx],\n",
    "                                        'normalized_bbox',\n",
    "                                        normalize_rect_vertex(prediction_instances.iloc[row_idx]['bbox'], image_size))\n",
    "    #prediction_instances['normalized_bbox'] = prediction_instances['bbox'].map(normalize_rect_vertex, arg= (image_size))\n",
    "\n",
    "\n",
    "def pair_gene(startor, startor_neighbor, receptor, receptor_neighbor, text_instances):\n",
    "\n",
    "    assert receptor is not None\n",
    "    assert startor is not None\n",
    "    assert 'ocr' in text_instances.columns\n",
    "\n",
    "    dist_ar = dist_center(startor, receptor)\n",
    "\n",
    "    if startor_neighbor is None or \\\n",
    "            dist_center(startor_neighbor, startor) <= 0.1 * dist_ar:\n",
    "        startor_neighbor = receptor\n",
    "\n",
    "\n",
    "    if receptor_neighbor is None or \\\n",
    "            dist_center(receptor_neighbor, receptor) <= 0.1 * dist_ar:\n",
    "        receptor_neighbor = startor\n",
    "\n",
    "    best_startor_index = \\\n",
    "        find_best_text(startor, text_instances['perspective_bbox'], startor_neighbor, receptor)\n",
    "\n",
    "    best_receptor_index = \\\n",
    "        find_best_text(receptor, text_instances['perspective_bbox'], receptor_neighbor, startor)\n",
    "\n",
    "    if best_startor_index is not None and best_receptor_index is not None:\n",
    "        dist_text = dist_center(center_point_in_box(text_instances.iloc[best_startor_index]['perspective_bbox']),\n",
    "                                center_point_in_box(text_instances.iloc[best_receptor_index]['perspective_bbox']))\n",
    "\n",
    "        if best_receptor_index != best_startor_index and dist_text > dist_ar * 0.8:\n",
    "            return  text_instances.iloc[best_startor_index]['ocr'], \\\n",
    "                    text_instances.iloc[best_receptor_index]['ocr'], \\\n",
    "                    text_instances.iloc[best_startor_index]['perspective_bbox'], \\\n",
    "                    text_instances.iloc[best_receptor_index]['perspective_bbox']\n",
    "        else:\n",
    "            raise Exception('startor and receptor match to a same gene')\n",
    "    else:\n",
    "        raise Exception('cannot match startor or receptor')\n",
    "\n",
    "\n",
    "\n",
    "# generate sub_image and fill entity bounding boxes for regular bbox\n",
    "def generate_sub_image_bounding_relation_regular(img, relation_instance, element_instances_on_sample, offset):\n",
    "\n",
    "    # image_name, image_ext = os.path.splitext(os.path.basename(img_file_name))\n",
    "    # element_boxes= []\n",
    "    # for idx in relation_instance['cover_entity']:\n",
    "    #     element_boxes.append(entity_instances.iloc[idx]['normalized_bbox'])\n",
    "\n",
    "    src_pts = relation_instance['normalized_bbox']\n",
    "\n",
    "    # get all element instances on relation region\n",
    "    try:\n",
    "        element_instances_on_relation = element_instances_on_sample.iloc[relation_instance['covered_elements']].copy()\n",
    "\n",
    "        # get bbox after perspective transform\n",
    "        element_instances_on_relation['perspective_bbox'] = element_instances_on_relation['normalized_bbox'].apply(\n",
    "            translation_transform_on_element_bbox, M=src_pts[0])\n",
    "\n",
    "    except:\n",
    "        print('element_instances_on_sample:', element_instances_on_sample)\n",
    "\n",
    "    # directly warp the rotated rectangle to get the straightened rectangle\n",
    "    warped_img = get_subimg(img, src_pts, offset)\n",
    "\n",
    "\n",
    "    return warped_img, element_instances_on_relation\n",
    "\n",
    "# generate sub_image and fill entity bounding boxes\n",
    "def generate_sub_image_bounding_relation_rotated(img, relation_instance, element_instances_on_sample, offset):\n",
    "\n",
    "    # image_name, image_ext = os.path.splitext(os.path.basename(img_file_name))\n",
    "\n",
    "    # element_boxes= []\n",
    "    # for idx in relation_instance['cover_entity']:\n",
    "    #     element_boxes.append(entity_instances.iloc[idx]['normalized_bbox'])\n",
    "\n",
    "    src_pts = relation_instance['normalized_bbox']\n",
    "\n",
    "    # corrdinate of the points in box points after the rectangle has been\n",
    "    # straightened\n",
    "    dst_pts = np.array([[0, relation_instance['bbox'][3]-offset],\n",
    "                        [0, 0],\n",
    "                        [relation_instance['bbox'][2]-offset, 0],\n",
    "                        [relation_instance['bbox'][2]-offset,\n",
    "                         relation_instance['bbox'][3]-offset]], dtype= np.float32)\n",
    "\n",
    "    # the perspective transformation matrix\n",
    "    transform = cv2.getPerspectiveTransform(src_pts.astype(np.float32), dst_pts)\n",
    "    # get all element instances on relation region\n",
    "    element_instances_on_relation = element_instances_on_sample.iloc[relation_instance['covered_elements']].copy()\n",
    "\n",
    "    # get bbox after perspective transform\n",
    "    element_instances_on_relation['perspective_bbox'] = element_instances_on_relation['normalized_bbox'].apply(perspective_transform_on_element_bbox, M =transform)\n",
    "\n",
    "    # directly warp the rotated rectangle to get the straightened rectangle\n",
    "    warped_img = cv2.warpPerspective(img, transform, (int(relation_instance['bbox'][2]),\n",
    "                                                      int(relation_instance['bbox'][3])))\n",
    "\n",
    "    return warped_img, element_instances_on_relation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_relation_sub_image_and_pairing(img,image_name,image_ext,relation_instances_on_sample,element_instances_on_sample,relation_type,subimage_path):\n",
    "    sub_image_path= os.path.join(subimage_path,'sub_image')\n",
    "    paired_image_path=os.path.join(subimage_path,'paired')\n",
    "    if not os.path.exists(sub_image_path):\n",
    "        os.mkdir(sub_image_path)\n",
    "\n",
    "    if not os.path.exists(paired_image_path):\n",
    "        os.mkdir(paired_image_path)\n",
    "\n",
    "    for relation_index in range(0, len(relation_instances_on_sample)):\n",
    "        # plot covered elements by this relation at whole image\n",
    "        element_instances_on_relation = element_instances_on_sample.iloc[relation_instances_on_sample.iloc[relation_index]['covered_elements']].copy()\n",
    "        # print('element_instances_on_relation',element_instances_on_relation)\n",
    "        covered_element_bboxes = element_instances_on_sample.iloc[relation_instances_on_sample.iloc[relation_index]['covered_elements']]['normalized_bbox']\n",
    "\n",
    "        if relation_type =='relation':\n",
    "            #regular bbox\n",
    "            sub_img, element_instances_on_relation = generate_sub_image_bounding_relation_regular(img,\n",
    "                                                 relation_instances_on_sample.iloc[relation_index],\n",
    "                                                 element_instances_on_sample, offset= 1)\n",
    "            # print('element_instances_on_relation',element_instances_on_relation)\n",
    "\n",
    "        elif relation_type =='rotated_relation':\n",
    "            # #rotated bbox\n",
    "            sub_img, element_instances_on_relation = generate_sub_image_bounding_relation_rotated(img,\n",
    "                                                  relation_instances_on_sample.iloc[relation_index],\n",
    "                                                  element_instances_on_sample,  offset= 1)\n",
    "\n",
    "        # plot elements on sub_img using their perspective\n",
    "\n",
    "        sub_img_copy = sub_img.copy()\n",
    "        # for element_idx in range(0, len(element_instances_on_relation)):\n",
    "        #     if element_instances_on_relation.iloc[element_idx]['category_id'] == cfg.element_list.index('gene'):\n",
    "        #         cv2.polylines(sub_img_copy, [element_instances_on_relation.iloc[element_idx]['perspective_bbox']], isClosed=True,\n",
    "        #                       color=[255, 0, 0], thickness=2)\n",
    "        #     if element_instances_on_relation.iloc[element_idx]['category_id'] == cfg.element_list.index('gene'):\n",
    "        #         cv2.putText(sub_img_copy, element_instances_on_relation.iloc[element_idx]['ocr'],\n",
    "        #                     tuple(element_instances_on_relation.iloc[element_idx]['perspective_bbox'][0]),\n",
    "        #                     fontFace= cv2.FONT_HERSHEY_COMPLEX_SMALL, fontScale= 2, color =(0,255,0), thickness= 2)\n",
    "\n",
    "        # # save sub-image to visualize sub-img\n",
    "        cv2.imwrite(os.path.join(sub_image_path,image_name + str(relation_index) + image_ext), sub_img_copy)\n",
    "\n",
    "\n",
    "        # do gene pairing\n",
    "        startor, receptor = get_gene_pairs_on_relation_sub_image( sub_img, paired_image_path, \n",
    "            element_instances_on_relation= element_instances_on_relation, image_name=image_name, \n",
    "            image_ext=image_ext, idx=relation_index)\n",
    "        # startor_index=np.where(element_instances_on_sample.normalized_bbox==(element_instances_on_sample.iloc[12]['normalized_bbox']))\n",
    "\n",
    "        #update paired results into relation_instances_on_sample\n",
    "        if startor is not None and receptor is not None:\n",
    "            relation_instances_on_sample.at[relation_index, 'startor'] = startor\n",
    "            relation_instances_on_sample.at[relation_index,'relation_category'] = cfg.relation_list[relation_instances_on_sample.iloc[relation_index]['category_id']]\n",
    "            relation_instances_on_sample.at[relation_index, 'receptor'] = receptor\n",
    "\n",
    "        del sub_img, element_instances_on_relation\n",
    "\n",
    "    return relation_instances_on_sample\n",
    "\n",
    "\n",
    "\n",
    "def assign_roles_to_elements(gene_instances_on_sub_image, relation_head_instance_on_sub_image):\n",
    "    assert len(gene_instances_on_sub_image) == 2\n",
    "    # print('gene_instances_on_sub_image',gene_instances_on_sub_image)\n",
    "    # print('relation_head_instance_on_sub_image',relation_head_instance_on_sub_image)\n",
    "    element_distance0 = calculate_distance_between_two_boxes(relation_head_instance_on_sub_image['perspective_bbox'], gene_instances_on_sub_image.iloc[0]['perspective_bbox'])\n",
    "    element_distance1 = calculate_distance_between_two_boxes(relation_head_instance_on_sub_image['perspective_bbox'], gene_instances_on_sub_image.iloc[1]['perspective_bbox'])\n",
    "\n",
    "    if element_distance0 > element_distance1:\n",
    "        #return gene_instances_on_sub_image.iloc[0]['ocr'] + '<' + relation_head_instance_on_sub_image['category'] + '>' + gene_instances_on_sub_image.iloc[1]['ocr'], \\\n",
    "        #return gene_instances_on_sub_image.iloc[0]['perspective_bbox'], gene_instances_on_sub_image.iloc[1]['perspective_bbox']\n",
    "        return gene_instances_on_sub_image.iloc[0]['ocr'], gene_instances_on_sub_image.iloc[1]['ocr'], \\\n",
    "                gene_instances_on_sub_image.iloc[0]['perspective_bbox'], gene_instances_on_sub_image.iloc[1]['perspective_bbox'],\n",
    "\n",
    "    else:\n",
    "        #return gene_instances_on_sub_image.iloc[1]['ocr'] + '<' + relation_head_instance_on_sub_image['category'] + '>' + gene_instances_on_sub_image.iloc[0]['ocr'], \\\n",
    "        #return gene_instances_on_sub_image.iloc[1]['perspective_bbox'], gene_instances_on_sub_image.iloc[0]['perspective_bbox']\n",
    "        return gene_instances_on_sub_image.iloc[1]['ocr'], gene_instances_on_sub_image.iloc[0]['ocr'], \\\n",
    "               gene_instances_on_sub_image.iloc[1]['perspective_bbox'], gene_instances_on_sub_image.iloc[0][\n",
    "                   'perspective_bbox'],\n",
    "\n",
    "\n",
    "def assign_roles_to_elements_body(gene_instances_on_sub_image, relation_head_instance_on_sub_image):\n",
    "    assert len(gene_instances_on_sub_image) == 2\n",
    "    # print('gene_instances_on_sub_image',gene_instances_on_sub_image)\n",
    "    # print('relation_head_instance_on_sub_image',relation_head_instance_on_sub_image)\n",
    "    # element_distance0 = calculate_distance_between_two_boxes(relation_head_instance_on_sub_image['perspective_bbox'], gene_instances_on_sub_image.iloc[0]['perspective_bbox'])\n",
    "    # element_distance1 = calculate_distance_between_two_boxes(relation_head_instance_on_sub_image['perspective_bbox'], gene_instances_on_sub_image.iloc[1]['perspective_bbox'])\n",
    "    center0_x, center0_y = center_point_in_box(gene_instances_on_sub_image.iloc[0]['normalized_bbox'])\n",
    "    center1_x, center1_y = center_point_in_box(gene_instances_on_sub_image.iloc[1]['normalized_bbox'])\n",
    "    center2_x = relation_head_instance_on_sub_image['head'][0]\n",
    "    center3_x = relation_head_instance_on_sub_image['tail'][0]\n",
    "    center2_y = relation_head_instance_on_sub_image['head'][1]\n",
    "    center3_y = relation_head_instance_on_sub_image['tail'][1]\n",
    "    # print(center0_x, center0_y,center1_x, center1_y)\n",
    "    element_distance_head0 = np.sqrt((center0_x - center2_x) ** 2 + (center0_y - center2_y) ** 2)\n",
    "    element_distance_tail0 = np.sqrt((center0_x - center3_x) ** 2 + (center0_y - center3_y) ** 2)\n",
    "    element_distance_head1 = np.sqrt((center1_x - center2_x) ** 2 + (center1_y - center2_y) ** 2)\n",
    "    element_distance_tail1 = np.sqrt((center1_x - center3_x) ** 2 + (center1_y - center3_y) ** 2)\n",
    "    # print(element_distance_head0 ,element_distance_head1, element_distance_tail0, element_distance_tail1)\n",
    "\n",
    "    if  element_distance_head0 > element_distance_tail0 and element_distance_head1 < element_distance_tail1 :\n",
    "        #return gene_instances_on_sub_image.iloc[0]['ocr'] + '<' + relation_head_instance_on_sub_image['category'] + '>' + gene_instances_on_sub_image.iloc[1]['ocr'], \\\n",
    "        #return gene_instances_on_sub_image.iloc[0]['perspective_bbox'], gene_instances_on_sub_image.iloc[1]['perspective_bbox']\n",
    "        return gene_instances_on_sub_image.iloc[0]['ocr'], gene_instances_on_sub_image.iloc[1]['ocr'], \\\n",
    "                gene_instances_on_sub_image.iloc[0]['normalized_bbox'], gene_instances_on_sub_image.iloc[1]['normalized_bbox'],\n",
    "\n",
    "    else:\n",
    "        #return gene_instances_on_sub_image.iloc[1]['ocr'] + '<' + relation_head_instance_on_sub_image['category'] + '>' + gene_instances_on_sub_image.iloc[0]['ocr'], \\\n",
    "        #return gene_instances_on_sub_image.iloc[1]['perspective_bbox'], gene_instances_on_sub_image.iloc[0]['perspective_bbox']\n",
    "        return gene_instances_on_sub_image.iloc[1]['ocr'], gene_instances_on_sub_image.iloc[0]['ocr'], \\\n",
    "               gene_instances_on_sub_image.iloc[1]['normalized_bbox'], gene_instances_on_sub_image.iloc[0][\n",
    "                   'normalized_bbox'],\n",
    "\n",
    "# takes sub_image_filenames and predicted classes and extracts the relationship type and pairs\n",
    "# returns entity pairs in list of tuples and list of strings (format: \"relationship_type:starter|receptor\")\n",
    "def get_gene_pairs_on_relation_sub_image (sub_img,subimage_path, element_instances_on_relation,image_name, image_ext, idx):\n",
    "\n",
    "    #analyze the element distribution first\n",
    "    gene_instances_on_relation = element_instances_on_relation.loc[element_instances_on_relation['category_id'] == cfg.element_list.index('gene')]\n",
    "    relation_symbol_instances_on_relation = element_instances_on_relation.loc[element_instances_on_relation['category_id'] != cfg.element_list.index('gene')]\n",
    "\n",
    "    #pick the mostlikely relation symbols if more than 1 relation symbol\n",
    "    relation_head_instance, relation_symbol_contour = \\\n",
    "    find_largest_area_symbols(sub_img, gene_instances_on_relation,relation_symbol_instances_on_relation)\n",
    "\n",
    "    # print('relation_head_instance',relation_head_instance)\n",
    "    # print('relation_symbol_contour',relation_symbol_contour)\n",
    "\n",
    "\n",
    "    #if more than 2 genes\n",
    "    if len(gene_instances_on_relation) > 2:\n",
    "        # TODO alternative strategy 1: cluster then into 2 groups\n",
    "\n",
    "        # ongoing strategy: from the relation symbol to find the closest 2 genes\n",
    "        vertex_candidates = cv2.approxPolyDP(relation_symbol_contour, epsilon=5, closed=True)\n",
    "\n",
    "        startor_point, startor_neighbor, receptor_point, receptor_neighbor = \\\n",
    "        find_vertex_for_detected_relation_symbol_by_distance(sub_img, vertex_candidates, relation_head_instance['normalized_bbox'])\n",
    "\n",
    "        try:\n",
    "            startor, receptor, startor_bbox, receptor_bbox = \\\n",
    "                pair_gene(startor_point, startor_neighbor, receptor_point, receptor_neighbor, gene_instances_on_relation)\n",
    "\n",
    "            #just for visualization\n",
    "            # cv2.polylines(sub_img, [startor_bbox], isClosed= True,  color=(0, 255, 0), thickness= 2)\n",
    "            # cv2.polylines(sub_img, [receptor_bbox], isClosed= True, color=(0, 0, 255), thickness=2)\n",
    "            # cv2.imwrite(os.path.join(subimage_path,image_name + '_' + str(idx) + image_ext), sub_img)\n",
    "\n",
    "            return startor, receptor\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            return None, None\n",
    "\n",
    "    # at last leave only 2 genes/groups and 1\n",
    "    else:\n",
    "        startor,receptor, startor_bbox, receptor_bbox = \\\n",
    "        assign_roles_to_elements_body(gene_instances_on_sub_image= gene_instances_on_relation,\n",
    "                                 relation_head_instance_on_sub_image= relation_head_instance)\n",
    "        # cv2.polylines(sub_img, [startor_bbox], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "        # cv2.polylines(sub_img, [receptor_bbox], isClosed=True, color=(0, 0, 255), thickness=2)\n",
    "        # cv2.imwrite(os.path.join(subimage_path  , image_name + '_' + str(idx) + image_ext), sub_img)\n",
    "        return startor, receptor\n",
    "\n",
    "\n",
    "def compute_iou(box1, box2, wh=True):\n",
    "    \"\"\"\n",
    "    compute the iou of two boxes.\n",
    "    Args:\n",
    "        box1, box2: [xmin, ymin, xmax, ymax] (wh=False) or [xcenter, ycenter, w, h] (wh=True)\n",
    "        wh: the format of coordinate.\n",
    "    Return:\n",
    "        iou: iou of box1 and box2.\n",
    "    \"\"\"\n",
    "    if wh == False:\n",
    "        xmin1, ymin1, xmax1, ymax1 = box1\n",
    "        xmin2, ymin2, xmax2, ymax2 = box2\n",
    "    else:\n",
    "        xmin1, ymin1 = int(box1[0]), int(box1[1])\n",
    "        xmax1, ymax1 = int(box1[0]+box1[2]), int(box1[1]+box1[3])\n",
    "        xmin2, ymin2 = int(box2[0]), int(box2[1])\n",
    "        xmax2, ymax2 = int(box2[0]+box2[2]), int(box2[1]+box2[3])\n",
    "        # xmin1, ymin1 = int(box1[0]-box1[2]/2.0), int(box1[1]-box1[3]/2.0)\n",
    "        # xmax1, ymax1 = int(box1[0]+box1[2]/2.0), int(box1[1]+box1[3]/2.0)\n",
    "        # xmin2, ymin2 = int(box2[0]-box2[2]/2.0), int(box2[1]-box2[3]/2.0)\n",
    "        # xmax2, ymax2 = int(box2[0]+box2[2]/2.0), int(box2[1]+box2[3]/2.0)\n",
    "        center = [(xmax1+xmin1)/2,(ymax1+ymin1)/2]\n",
    "\n",
    "    ## 获取矩形框交集对应的左上角和右下角的坐标（intersection）\n",
    "    xx1 = np.max([xmin1, xmin2])\n",
    "    yy1 = np.max([ymin1, ymin2])\n",
    "    xx2 = np.min([xmax1, xmax2])\n",
    "    yy2 = np.min([ymax1, ymax2])\n",
    "    ## 计算两个矩形框面积\n",
    "    area1 = (xmax1-xmin1) * (ymax1-ymin1)\n",
    "    area2 = (xmax2-xmin2) * (ymax2-ymin2)\n",
    "    inter_area = (np.max([0, xx2-xx1])) * (np.max([0, yy2-yy1]))\n",
    "    iou = inter_area / (area1+area2-inter_area+1e-6)\n",
    "    # print('iou,center',iou,center)\n",
    "    return iou,center\n",
    "\n",
    "def compute_dis(point1,point2):\n",
    "    dis  = np.sqrt((point2[0]-point1[0]) ** 2 + (point2[1]-point1[1]) ** 2)\n",
    "    return dis    \n",
    "\n",
    "def run_model(cfg, relation_h, **kwargs):\n",
    "    with open(cfg.dictionary_path) as gene_name_list_fp:\n",
    "        gene_name_list = json.load(gene_name_list_fp)\n",
    "\n",
    "    configuration = setup(cfg, kwargs)\n",
    "\n",
    "    model = RegularTrainer.build_model(configuration)\n",
    "\n",
    "    DetectionCheckpointer(model=model,\n",
    "                          save_dir=configuration.OUTPUT_DIR).resume_or_load(cfg.element_model, resume=False)\n",
    "\n",
    "    data_folder = os.path.join(kwargs['dataset'], 'img/')\n",
    "    ocr_sub_img_folder = os.path.join(data_folder, 'ocr_sub_img')\n",
    "    if not os.path.isdir(ocr_sub_img_folder):\n",
    "        os.mkdir(ocr_sub_img_folder)\n",
    "\n",
    "    data_loader = build_data_fold_loader(configuration, data_folder, mapper=DatasetMapper(configuration, False))\n",
    "  \n",
    "    img_size = {}\n",
    "    img_size['image_size'] = [data_loader.dataset[0]['height'], data_loader.dataset[0]['width']]\n",
    "    # print('data_loader.dataset[0]',data_loader.dataset[0])\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    predictions = []\n",
    "    with inference_context(model.to(device)), torch.no_grad():\n",
    "        for idx, inputs in enumerate(data_loader):\n",
    "            # print('&&&&&',idx,inputs)\n",
    "            output = model.to(device)(inputs)\n",
    "            instances = output[0][\"instances\"].to(cpu_device)\n",
    "            predictions = instances_to_coco_json(instances, inputs[0][\"image_id\"], inputs[0]['file_name'])\n",
    "    # predictions = inference_on_dataset(model, data_loader)\n",
    "\n",
    "            element_instances = pd.DataFrame(predictions)\n",
    "            # print('element_instances',element_instances)\n",
    "            element_instances['ocr'] = None\n",
    "            element_instances['head'] = None\n",
    "            element_instances['tail'] = None\n",
    "\n",
    "            element_instances_on_samples = element_instances.loc[(element_instances['score'] >= cfg.element_threshold) \\\n",
    "                                                                 & (element_instances['category_id'] == cfg.element_list.index(\n",
    "                'gene'))]\n",
    "\n",
    "            relation_symbol_instances_on_samples = element_instances.loc[(element_instances['score'] >= cfg.element_threshold) \\\n",
    "                                                                         & (element_instances[\n",
    "                                                                                'category_id'] != cfg.element_list.index(\n",
    "                'gene'))]\n",
    "            # print('relation_symbol_instances_on_samples', relation_symbol_instances_on_samples)\n",
    "          \n",
    "            # do OCR\n",
    "            file_list = set(element_instances_on_samples['file_name'])\n",
    "            for file_name in file_list:\n",
    "                image_name, _ = os.path.splitext(os.path.basename(file_name))\n",
    "                print('doing ocr to file {:s}'.format(file_name))\n",
    "                # img_id = element_instances_on_samples.loc[element_instances['file_name'] == file_name]['image_id'].iloc[0]\n",
    "                img_id = \\\n",
    "                element_instances_on_samples[element_instances_on_samples['file_name'] == file_name]['image_id'].values[0]\n",
    "\n",
    "                element_instances_on_sample = element_instances_on_samples.loc[element_instances_on_samples['file_name'] == file_name]\n",
    "\n",
    "                # ocr_results, all_results_dict, corrected_results_dict, fuzz_ratios_dict, coordinates_list, element_idx_list = \\\n",
    "                #  OCR(file_name, ocr_sub_img_folder, element_instances_on_sample, user_words=gene_name_list)\n",
    "                # print('coordinates_list',coordinates_list)\n",
    "                ocr_results, coordinates_list = gcv_ocr(file_name)\n",
    "                # postprocessing\n",
    "                # nfkc->deburr->upper->expand->swap\n",
    "\n",
    "                postprocessing_ocr_results = []\n",
    "                for r in ocr_results:\n",
    "                    r = nfkc(r)\n",
    "                    r = ''.join(r)\n",
    "                    r = deburr(r)\n",
    "                    r = ''.join(r)\n",
    "                    r = upper(r)\n",
    "                    r = ''.join(r)\n",
    "                    r = expand(r)\n",
    "                    r = ''.join(r)\n",
    "                    pp_r = swaps(r)\n",
    "                    pp_r = ''.join(pp_r)\n",
    "\n",
    "                    postprocessing_ocr_results.append(pp_r)\n",
    "\n",
    "                # print(\"\\nocr_results\\n\",ocr_results)\n",
    "                # print('\\npostprocessing_ocr_results\\n',postprocessing_ocr_results)\n",
    "\n",
    "\n",
    "                ocr_prediction_results = []\n",
    "                for k in range(1, len(postprocessing_ocr_results)):\n",
    "                    result = {\n",
    "                        \"image_id\": img_id,\n",
    "                        \"file_name\": file_name,\n",
    "                        \"category_id\": 1,\n",
    "                        \"bbox\": BoxMode.convert(np.array([coordinates_list[k][0], coordinates_list[k][2]]).reshape((-1, 4)),BoxMode.XYXY_ABS, BoxMode.XYWH_ABS).tolist()[0],\n",
    "                        \"score\": float(1),\n",
    "                        \"ocr\": postprocessing_ocr_results[k]\n",
    "                    }\n",
    "                    ocr_prediction_results.append(result)\n",
    "\n",
    "                new_element_instances_on_samples = pd.DataFrame(ocr_prediction_results)\n",
    "                # new_element_instances_on_samples['bbox'] = element_instances['bbox']\n",
    "                # print('new_element_instances_on_samples',new_element_instances_on_samples['bbox'])\n",
    "                # for i in range(0,len(new_element_instances_on_samples)):\n",
    "                #     new_element_instances_on_samples['bbox'][i][2]= new_element_instances_on_samples['bbox'][i][2] - new_element_instances_on_samples['bbox'][i][0]\n",
    "                #     new_element_instances_on_samples['bbox'][i][3]= new_element_instances_on_samples['bbox'][i][3] - new_element_instances_on_samples['bbox'][i][1]\n",
    "\n",
    "                relation_symbol_instances_on_samples = pd.concat([new_element_instances_on_samples, relation_symbol_instances_on_samples],\n",
    "                                              ignore_index=True)\n",
    "                # print('relation_symbol_instances_on_samples',relation_symbol_instances_on_samples)\n",
    "                # plot ocr results on images\n",
    "                # current_img = cv2.imread(file_name)\n",
    "                # for result_idx in range(1, len(ocr_results)):\n",
    "                #     cv2.putText(current_img, ocr_results[result_idx].encode('utf-8').decode('utf-8'),\n",
    "                #                 tuple(np.array(coordinates_list[result_idx][0], np.int)),\n",
    "                #                 cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "                #                 1, (0, 0, 255), 1)\n",
    "                #     # add ocr results to dataframe of element prediction results\n",
    "                #     # set_ocr_results_to_element_instance_df(element_instances,\n",
    "                #     #                                        element_instances_on_sample.iloc[element_idx_list[result_idx]]['bbox'],\n",
    "                #     #                                        results[result_idx])\n",
    "                # img_name, img_ext = os.path.splitext(file_name)\n",
    "                # cv2.imwrite(img_name + '_ocr' + img_ext, current_img)\n",
    "                # del current_img\n",
    "\n",
    "                # save results\n",
    "                json_dicts = []\n",
    "                json_dicts.append(img_size)\n",
    "                for i in range(1, len(postprocessing_ocr_results)):\n",
    "                    json_dict = {}\n",
    "                    json_dict['gene_name'] = postprocessing_ocr_results[i]\n",
    "                    json_dict['coordinates'] = \\\n",
    "                    BoxMode.convert(np.array([coordinates_list[i][0], coordinates_list[i][2]]).reshape((-1, 4)),\n",
    "                                    BoxMode.XYXY_ABS, BoxMode.XYWH_ABS).tolist()[0]\n",
    "                    # add ocr results to dataframe of element prediction results\n",
    "                    # set_ocr_results_to_element_instance_df(element_instances, element_instances_on_sample.iloc[element_idx_list[i]]['bbox'], results[i])\n",
    "                    json_dicts.append(json_dict)\n",
    "                # with open(data_folder + 'output-1.json', 'w+', encoding='utf-8') as file:\n",
    "                #     json.dump(json_dicts, file, ensure_ascii=False)\n",
    "                # with open(data_folder + 'output-1.json', 'w+', encoding='utf-8') as file:\n",
    "                #     json.dump(json_dicts, file)\n",
    "                with open(data_folder + '{:s}_elements.json'.format(image_name), 'w+', encoding='utf-8') as file:\n",
    "                    json.dump(json_dicts, file)\n",
    "\n",
    "                del ocr_results, coordinates_list, json_dicts,postprocessing_ocr_results\n",
    "\n",
    "            element_instances = relation_symbol_instances_on_samples\n",
    "            relation_subimage_path = os.path.join(data_folder, 'relation_subimage')\n",
    "            if not os.path.isdir(relation_subimage_path):\n",
    "                os.mkdir(relation_subimage_path)\n",
    "\n",
    "            element_instances['normalized_bbox'] = None\n",
    "            element_instances['center'] = None\n",
    "            element_instances['startor'] = None\n",
    "            element_instances['receptor'] = None\n",
    "            element_instances[\"relation_category\"] = None\n",
    "            # todo: organize results from one image as a group and sort by their scores\n",
    "            # element_instances.sort_values(by='score', ascending=False, inplace=True)\n",
    "\n",
    "            # run relation prediction\n",
    "            # relation_predictions = predict(cfg.relation_config_file, 'relation', data_folder)\n",
    "            # # print('relation_predictions',relation_predictions[0])\n",
    "            # # read the relations according to the same image\n",
    "            # relation_instances = pd.DataFrame(relation_predictions)\n",
    "            # print('relation_instances',relation_instances)\n",
    "            # add startor and receptor columns into relation_instances\n",
    "\n",
    "            # relation_instances['normalized_bbox'] = None\n",
    "            # relation_instances['startor'] = None\n",
    "            # relation_instances['relation_category'] = None\n",
    "            # relation_instances['receptor'] = None\n",
    "\n",
    "            image_file_list = set(element_instances['file_name'])\n",
    "            for current_image_file in image_file_list:\n",
    "                image_name, image_ext = os.path.splitext(os.path.basename(current_image_file))\n",
    "                element_instances_on_sample = element_instances[(element_instances['file_name'] == current_image_file) &\n",
    "                                                                (element_instances['score'] >= cfg.element_threshold)]\n",
    "                # print('element_instances_on_sample\\n',element_instances_on_sample)\n",
    "                # relation_instances_on_sample = relation_instances[(relation_instances['file_name'] == current_image_file) &\n",
    "                #                                                   (relation_instances['score'] >= cfg.relation_threshold)]\n",
    "                #find gene center \n",
    "                gene_dic = element_instances[(element_instances['file_name'] == current_image_file)&(element_instances['category_id']==1)]\n",
    "                gene_list = gene_dic['bbox'].tolist()\n",
    "                for i in range(0,len(gene_list)):\n",
    "                    center = [int(gene_list[i][0]+gene_list[i][2]/2),int(gene_list[i][1]+gene_list[i][3]/2)]\n",
    "                    element_instances_on_sample['center'][gene_dic.index[i]] = center\n",
    "\n",
    "                # find relation head\n",
    "                relationhead = relation_h[(relation_h['file_name'] == current_image_file)]\n",
    "\n",
    "                relation_head = relationhead['bbox'].tolist()\n",
    "                element_relation = element_instances[(element_instances['file_name'] == current_image_file)&(element_instances['category_id']!=1)]\n",
    "                relation_body = element_relation['bbox'].tolist()\n",
    "                # print(relation_body)\n",
    "                for i in range(0,len(relation_body)):\n",
    "                    iou=0\n",
    "                    temp_iou=0\n",
    "                    for j in range(0,len(relation_head)):\n",
    "                        # print('relation_head[i],relation_body[j]',relation_head[i],relation_body[j])\n",
    "                        temp_iou,center = compute_iou(relation_head[j],relation_body[i],True)\n",
    "                        if temp_iou>iou:\n",
    "                            iou = temp_iou\n",
    "                            element_instances_on_sample['head'][element_relation.index[i]] = center\n",
    "                        # else:\n",
    "                        #     element_instances_on_sample['head'][element_relation.index[i]] = [0,0]\n",
    "                # print(\"iou\",center)\n",
    "                # print('compute after relation_symbol_instances_on_samples', relation_symbol_instances_on_samples)\n",
    "                \n",
    "                # find relation tail\n",
    "                # print('current_image_file',current_image_file)\n",
    "                img = cv2.imread(current_image_file)\n",
    "                for i in range(0,len(relation_body)):\n",
    "                    bbox = relation_body[i]\n",
    "                    dis_max = 0\n",
    "                    crop_img = img[int(bbox[1]):int(bbox[1]+bbox[3]), int(bbox[0]):int(bbox[0]+bbox[2])]\n",
    "                    if crop_img is not None:\n",
    "                    # cv2.imwrite('/mnt/detectron2/pathway_retinanet_weiwei_65k/test/crop_ok.jpg', crop_img)\n",
    "                        gray = cv2.cvtColor(crop_img,cv2.COLOR_BGR2GRAY)\n",
    "                        corners = cv2.goodFeaturesToTrack(gray,20,0.06,10)\n",
    "                        # print(type(corners))\n",
    "                        \n",
    "                        if type(corners) is not 'NoneType':\n",
    "                            # 返回的结果是[[ 311., 250.]] 两层括号的数组。\n",
    "                            # corners.tolist()\n",
    "                            # try:\n",
    "                            #     corners = np.int0(corners)\n",
    "                            # except:\n",
    "                            #     continue\n",
    "                            corners = np.int0(corners)\n",
    "                            # print('!!!!!!!!!!!!!!!!!',corners)\n",
    "                            tail = []\n",
    "                            for j in corners:\n",
    "                                x,y = j.ravel()\n",
    "                                cv2.circle(crop_img,(x,y),3,255,-1)\n",
    "                                raw_x = x+bbox[0]\n",
    "                                raw_y = y+bbox[1]\n",
    "                                try:\n",
    "                                    head_x = element_instances_on_sample['head'][element_relation.index[i]][0]\n",
    "                                    head_y = element_instances_on_sample['head'][element_relation.index[i]][1]\n",
    "                                except:\n",
    "                                    continue\n",
    "                                dis = np.sqrt((raw_x - head_x) ** 2 + (raw_y - head_y) ** 2)\n",
    "                                if dis > dis_max:\n",
    "                                    dis_max = dis\n",
    "                                    tail = [raw_x,raw_y]\n",
    "\n",
    "                            element_instances_on_sample['tail'][element_relation.index[i]] = tail \n",
    "                            del tail\n",
    "                        else:\n",
    "                            element_instances_on_sample['tail'][element_relation.index[i]] = [0,0]\n",
    "                            # continue\n",
    "                # print('element_instances_on_sample\\n',element_instances_on_sample[['category_id','bbox','normalized_bbox','head','tail',\\\n",
    "                # \t'startor','receptor']])\n",
    "                # print('relation_instances_on_sample',relation_instances_on_sample)\n",
    "                img = cv2.imread(current_image_file)\n",
    "                height, width, _ = img.shape\n",
    "                # normalize_all_boxes(relation_instances_on_sample, (height, width))\n",
    "                normalize_all_boxes(element_instances_on_sample, (height, width))\n",
    "\n",
    "\n",
    "\n",
    "                #pairing\n",
    "                gene_e = element_instances_on_sample[element_instances['category_id']==1]\n",
    "                gene_element = gene_e['center'].tolist()\n",
    "                relation_e = element_instances_on_sample[element_instances['category_id']!=1]\n",
    "                relation_head = relation_e['head'].tolist()\n",
    "                relation_tail = relation_e['tail'].tolist()\n",
    "                print(gene_element)\n",
    "                print(relation_head)\n",
    "                print(relation_tail)\n",
    "\n",
    "                for i in range(0,len(relation_head)):\n",
    "                    dis_head = 1000\n",
    "                    for j in range(0,len(gene_element)):\n",
    "                        if relation_head[i]!= None and relation_head[i]!=[] and gene_element[j]!=None and gene_element[j]!=[]:\n",
    "                            dis = compute_dis(relation_head[i],gene_element[j])\n",
    "                            if dis<dis_head:\n",
    "                                dis_head = dis\n",
    "                                min_j = j\n",
    "                                ocr = element_instances_on_sample['ocr'][gene_e.index[j]]\n",
    "                    element_instances_on_sample['receptor'][relation_e.index[i]] = ocr\n",
    "\n",
    "                for i in range(0,len(relation_tail)):\n",
    "                    dis_tail = 1000\n",
    "                    for j in range(0,len(gene_element)):\n",
    "                        if relation_tail[i]!= None and relation_tail[i]!=[] and gene_element[j]!=None and gene_element[j]!=[]:\n",
    "                            dis = compute_dis(relation_tail[i],gene_element[j])\n",
    "                            if dis<dis_tail:\n",
    "                                dis_tail = dis\n",
    "                                min_j = j\n",
    "                                ocr = element_instances_on_sample['ocr'][gene_e.index[j]]\n",
    "                    element_instances_on_sample['startor'][relation_e.index[i]] = ocr\n",
    "\n",
    "                for i in range(0,len(element_instances_on_sample)):\n",
    "                    if element_instances_on_sample['category_id'][i]==0:\n",
    "                        element_instances_on_sample['relation_category'][i] = 'activate_relation'\n",
    "                    if element_instances_on_sample['category_id'][i]==2:\n",
    "                        element_instances_on_sample['relation_category'][i] = 'inhibit_relation'\n",
    "                    # element_instances_on_sample['head'][i]\n",
    "                    # element_instances_on_sample['tail'][i]\n",
    "                print('element_instances_on_sample\\n',element_instances_on_sample[['relation_category','ocr','normalized_bbox','center','head','tail',\\\n",
    "                    'startor','receptor']])\n",
    "\n",
    "                # for i in range(0,len(relation_element)):\n",
    "                #     dis_tail = 1000\n",
    "                #     for j in range(0,len(gene_element)):\n",
    "                #         dis = compute_dis(relation_element[i]['tail'],gene_element[j]['center'])\n",
    "                #         if dis<dis_tail:\n",
    "                #             dis_tail = dis\n",
    "                #             min_j = j\n",
    "                #             ocr = gene_element[j]['ocr']\n",
    "                #         element_instances_on_sample['startor'][relation_element.index[i]] = ocr\n",
    "\n",
    "                        \n",
    "                # for rows in element_instances_on_sample.iterrows():\n",
    "                #     if rows['category_id'] != 1:\n",
    "                #     \tprint(rows)\n",
    "                #     # \n",
    "                # print(element_instances_on_sample[['category_id','bbox','normalized_bbox','head','tail',\\\n",
    "                # \t'startor','receptor']])\n",
    "                # visualize normalized bboxes to confirm detection results\n",
    "                img_copy = img.copy()\n",
    "                for element_idx in range(0, len(element_instances_on_sample)):\n",
    "                    if element_instances_on_sample.iloc[element_idx]['category_id'] == 0:\n",
    "                        if element_instances_on_sample.iloc[element_idx]['score'] >= cfg.element_threshold:\n",
    "                            cv2.polylines(img_copy, [element_instances_on_sample.iloc[element_idx]['normalized_bbox']],\n",
    "                                          isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "                    elif element_instances_on_sample.iloc[element_idx]['category_id'] == 1:\n",
    "                        if element_instances_on_sample.iloc[element_idx]['score'] >= cfg.element_threshold:\n",
    "                            cv2.polylines(img_copy, [element_instances_on_sample.iloc[element_idx]['normalized_bbox']],\n",
    "                                          isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "                    elif element_instances_on_sample.iloc[element_idx]['category_id'] == 2:\n",
    "                        if element_instances_on_sample.iloc[element_idx]['score'] >= cfg.element_threshold:\n",
    "                            cv2.polylines(img_copy, [element_instances_on_sample.iloc[element_idx]['normalized_bbox']],\n",
    "                                          isClosed=True, color=(0, 0, 255), thickness=2)\n",
    "                for i in range(0,len(element_instances_on_sample[element_instances_on_sample['category_id']!=1])):\n",
    "                    head = element_instances_on_sample[element_instances_on_sample['category_id']!=1]['head']\n",
    "                    element_head = head.tolist()\n",
    "                    tail = element_instances_on_sample[element_instances_on_sample['category_id']!=1]['tail']\n",
    "                    element_tail = tail.tolist()\n",
    "                    # print(element_head,element_tail)\n",
    "                    if element_head[i]!=None and element_head[i]!=[] and element_tail[i]!=None and element_tail[i]!=[]:\n",
    "                        x_head = int(element_head[i][0])\n",
    "                        y_head = int(element_head[i][1])\n",
    "                        x_tail = int(element_tail[i][0])\n",
    "                        y_tail = int(element_tail[i][1])\n",
    "                        # print(x_head,y_head,x_tail,y_tail)\n",
    "                        cv2.circle(img_copy, (x_head,y_head), 6, (128,0,128), -1)\n",
    "                        cv2.circle(img_copy, (x_tail,y_tail), 6,(0,255,255), -1)\n",
    "                   \n",
    "                    # cv2.circle(img_copy, (element_instances_on_sample['head'][i][0],element_instances_on_sample['head'][i][1]), 6, (128,0,128), 0)\n",
    "                    # cv2.circle(img_copy, (element_instances_on_sample['tail'][i][0],element_instances_on_sample['tail'][i][1]), 6,(255,255,0), 0)\n",
    "                    # # img_copy(element_instances_on_sample['head'][i]) = [128,0,128] \n",
    "                    # img_copy(element_instances_on_sample['tail'][i]) = [255,255,0]\n",
    "                # print('!!!!!!element_instances_on_sample',element_instances_on_sample)\n",
    "                # for relation_idx in range(0, len(relation_instances_on_sample)):\n",
    "                #     if relation_instances_on_sample.iloc[relation_idx]['category_id']==0:\n",
    "                #         if relation_instances_on_sample.iloc[relation_idx]['score'] >= cfg.relation_threshold:\n",
    "                #             cv2.polylines(img_copy, [relation_instances_on_sample.iloc[relation_idx]['normalized_bbox']],\n",
    "                #                             isClosed=True, color=(255, 215, 0), thickness=2)\n",
    "                #     elif relation_instances_on_sample.iloc[relation_idx]['category_id']==1:\n",
    "                #         if relation_instances_on_sample.iloc[relation_idx]['score'] >= cfg.relation_threshold:\n",
    "                #             cv2.polylines(img_copy, [relation_instances_on_sample.iloc[relation_idx]['normalized_bbox']],\n",
    "                #                             isClosed=True, color=(128, 0, 128), thickness=2)\n",
    "\n",
    "                # print('*******************************',relation_instances_on_sample)\n",
    "                \n",
    "                cv2.imwrite(os.path.join(relation_subimage_path, image_name + image_ext), img_copy)\n",
    "                del img_copy\n",
    "            result = element_instances_on_sample[element_instances_on_sample['category_id']!=1]\n",
    "            results = result[[\"image_id\",\"file_name\",\"category_id\",\"bbox\",\"normalized_bbox\",\"startor\",\"relation_category\",\"receptor\"]]\n",
    "\n",
    "            \n",
    "            with open('{:s}_relation.json'.format(os.path.join(data_folder, image_name)), 'w') as output_fp:\n",
    "                        results.to_json(output_fp, orient='index')\n",
    "            del predictions, element_instances , image_file_list\n",
    "\n",
    "def run_model_head(cfg_head,  **kwargs):\n",
    "\n",
    "    configuration = setup(cfg_head, kwargs)\n",
    "\n",
    "    model = RegularTrainer.build_model(configuration)\n",
    "\n",
    "    DetectionCheckpointer(model=model,\n",
    "                          save_dir=configuration.OUTPUT_DIR).resume_or_load(cfg_head.element_model, resume=False)\n",
    "\n",
    "    # data_folder = r'/home/fei/Desktop/weiwei/data/use_case/test/0/'\n",
    "    # ocr_sub_img_folder=r'/home/fei/Desktop/weiwei/data/use_case/test/ocr_sub_img/pdf_170_Targeting_2_10/'\n",
    "    data_folder = os.path.join(kwargs['dataset'], 'img/')\n",
    "    # print('data_folder:', data_folder)\n",
    "\n",
    "    data_loader = build_data_fold_loader(configuration, data_folder, mapper=DatasetMapper(configuration, False))\n",
    "    img_size = {}\n",
    "    img_size['image_size'] = [data_loader.dataset[0]['height'], data_loader.dataset[0]['width']]\n",
    "\n",
    "    predictions = inference_on_dataset(model, data_loader)\n",
    "\n",
    "    element_instances = pd.DataFrame(predictions)\n",
    "    # print('element_instances',element_instances)\n",
    "    element_instances['ocr'] = None\n",
    "    # element_instances_on_samples = element_instances.loc[(element_instances['score'] >= cfg.element_threshold) \\\n",
    "    #                                                      & (element_instances['category_id'] == cfg.element_list.index(\n",
    "    #     'gene'))]\n",
    "    # print(' element_instances_on_samples', element_instances_on_samples)\n",
    "    relation_symbol_instances_on_samples = element_instances.loc[(element_instances['score'] >= cfg.element_threshold) \\\n",
    "                                                                 & (element_instances[\n",
    "                                                                        'category_id'] != cfg.element_list.index(\n",
    "        'gene'))]\n",
    "    # print('head_relation_symbol_instances_on_samples', relation_symbol_instances_on_samples)\n",
    "\n",
    "\n",
    "\n",
    "    return  relation_symbol_instances_on_samples\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset', type=str, default='', help='input data')\n",
    "    # parser.add_argument(\"--outputpath\", type=str, default=\"\", help=\"append to the dir name\")\n",
    "    # parser.add_argument(\"--cfg_file_path\", type=str, default='', help=\"cfg_file_path\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # args.outputpath = r'/home/fei/Desktop/test/results/'\n",
    "\n",
    "    args.dataset = r'/mnt/detectron2/pathway_retinanet_weiwei_65k'\n",
    "    # cfg_file_path = r'./Base-RetinaNet.yaml'\n",
    "\n",
    "    # for k, v in vars(args).items():\n",
    "    #     print(k, ':', v)\n",
    "\n",
    "    # file_path='/home/fei/Desktop/weiwei/pathway_web/SkyEye/users/upload-files/2020-04-21T22:35:29.355Z/'\n",
    "    file_path = vars(args)['dataset']\n",
    "    img_path = os.path.join(file_path, 'img/')\n",
    "    if not os.path.exists(img_path):\n",
    "        os.makedirs(img_path)\n",
    "\n",
    "    # with open(file_path+'input.txt','r') as file:\n",
    "    #     with open(img_path+'input.jpg','wb') as f:\n",
    "    #         print('img_path:', img_path+'input.jpg')\n",
    "    #\n",
    "    #         img_data = file.read().split(',',1)[1]\n",
    "    #         img=base64.b64decode(img_data)\n",
    "    #         f.write(img)\n",
    "    relation_head = run_model_head(cfg_head, **vars(args))\n",
    "    run_model(cfg, relation_head, **vars(args))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c303b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
